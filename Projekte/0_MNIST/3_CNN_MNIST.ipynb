{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neuronal Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "data_augmentation = True # Schalter für die Image Änderung => \n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Die Input Shape muss für den ErstenLayer vorverarbeitet sein\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\1810837475\\.conda\\envs\\testtensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "model = Path(\"./Data/modelcnn.json\")\n",
    "if model.is_file():\n",
    "   # load json and create model\n",
    "    json_file = open('./Data/modelcnn.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"./Data/modelcnn.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "else:\n",
    "    modelcnn = Sequential()\n",
    "    modelcnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape)) # Größe von Kernel/Filter / Activation / Input Layer\n",
    "    modelcnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    modelcnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    modelcnn.add(Dropout(0.25))\n",
    "    modelcnn.add(Flatten())\n",
    "    modelcnn.add(Dense(128, activation='relu'))\n",
    "    modelcnn.add(Dropout(0.5))\n",
    "    modelcnn.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\1810837475\\.conda\\envs\\testtensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 574s 10ms/step - loss: 0.2620 - accuracy: 0.9189 - val_loss: 0.0583 - val_accuracy: 0.9804\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 443s 7ms/step - loss: 0.0914 - accuracy: 0.9736 - val_loss: 0.0444 - val_accuracy: 0.9848\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 371s 6ms/step - loss: 0.0664 - accuracy: 0.9803 - val_loss: 0.0357 - val_accuracy: 0.9877\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 406s 7ms/step - loss: 0.0564 - accuracy: 0.9832 - val_loss: 0.0300 - val_accuracy: 0.9888\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 446s 7ms/step - loss: 0.0467 - accuracy: 0.9858 - val_loss: 0.0264 - val_accuracy: 0.9918\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 635s 11ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0306 - val_accuracy: 0.9896\n",
      "Epoch 7/20\n",
      "34176/60000 [================>.............] - ETA: 4:20 - loss: 0.0372 - accuracy: 0.9891"
     ]
    }
   ],
   "source": [
    "history = modelcnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# wird bei CNN für CIFAR \"ausgefeilter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelcnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = modelcnn.to_json()\n",
    "with open(\"./MODEL/modelcnn.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "modelcnn.save_weights(\"./MODEL/modelcnn.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load json and create model\n",
    "json_file = open('./MODEL/modelcnn.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "modelcnn = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "modelcnn.load_weights(\"./MODEL/modelcnn.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "modelcnn.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = modelcnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
