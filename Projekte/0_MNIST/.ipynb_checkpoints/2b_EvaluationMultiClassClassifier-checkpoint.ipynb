{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Algorithms for MNIST | Multiclass Classifier\n",
    "Dieses Notebook dient dazu alle Modelle miteinander zu vergleichen, dazu wurden die fertig trainierten Modelle in dem Directory ./MODEL abgelegt, hier in diesem Notebook werden die trainierten Modelle geladen und miteinander verglichen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.externals import joblib # Laden der Modelle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.model_selection import cross_val_predict # For Predictions\n",
    "from sklearn.metrics import confusion_matrix # For CM\n",
    "from sklearn.model_selection import cross_val_score # For Cross Validation\n",
    "from sklearn.metrics import roc_curve # Für die ROC-Kurve\n",
    "from sklearn.metrics import roc_auc_score # Für die AUC\n",
    "from IPython.display import HTML, display# darstellung der Ergebnisslisten\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler # Für das Skalieren von SGD\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten für die Predcitions\n",
    "train_X, train_y = loadlocal_mnist(\n",
    "        images_path='./Data/train-images.idx3-ubyte', \n",
    "        labels_path='./Data/train-labels.idx1-ubyte')\n",
    "test_X, test_y = loadlocal_mnist(\n",
    "        images_path='./Data/t10k-images.idx3-ubyte', \n",
    "        labels_path='./Data/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier lediglich die fertig trainierten Modelle laden\n",
    "sgd_clf = joblib.load(\"./MODEL/MNIST_MultiClassClassifier_sgd_clf.pkl\")\n",
    "logReg_clf = joblib.load(\"./MODEL/MNIST_MultiClassClassifier_logReg_clf.pkl\")\n",
    "forest_clf = joblib.load(\"./MODEL/MNIST_MultiClassClassifier_forest_clf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste aller Modelle, welche wir im weiteren auswerten\n",
    "trainedModels = [sgd_clf, logReg_clf, forest_clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "Bei der Accuracy misst man die Anzahl der richtig klassifizierten einheiten gegen alle \n",
    "Die Fragestellung lautet gegen was man testet\n",
    "1) gegen die Testdaten\n",
    "2) gegen die CV-Daten\n",
    "im Weiteren arbeit ich mit cv-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "accuracyList = [[\"Model\", \"Accuracys per CV\"]]\n",
    "for model in range(0, len(trainedModels),1):  \n",
    "    interim = []\n",
    "    interim.append(trainedModels[model])\n",
    "    interim.append(cross_val_score(trainedModels[model], train_X, train_y, cv=3, scoring=\"accuracy\"))\n",
    "    accuracyList.append(interim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\n",
    "   '<table border width=100% height=100><tr>{}</tr></table>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in accuracyList)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mögiche Verbesserungen für die Accuracy\n",
    "durch das Skalieren der Train-Daten kann bereits ein erheblich besseres Ergbenis erstellt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BSP:\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_X.astype(np.float64))\n",
    "print(cross_val_score(sgd_clf, X_train_scaled, train_y, cv=3, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit ist durch alleiniges Skalieren der Daten bereits eine höhere Accuracy erzielt worden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "Idee de Confusion Matrix = man zählt wie oft welche Klasse Richtig, oder eben Falsch klassifiziert wurde. \n",
    "hier haben wir die Ausgangslage, dass trainierte Modelle(auf Basis der Traindata) vorliegen. für das Testen gibt es nun zwei Möglichkeiten\n",
    "1) Auf Basis von Crossvalidation innerhalb des Train-Sets\n",
    "2) Auf Basis der Testdata\n",
    "Im weiteren Arbeiten wir mit der 1) version mit einer CV von 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in range(0, len(trainedModels),1):\n",
    "    y_train_pred = cross_val_predict(trainedModels[model], train_X, train_y, cv=3)\n",
    "    print(trainedModels[model])\n",
    "    conf_mx = (confusion_matrix(train_y, y_train_pred))\n",
    "    print(conf_mx)\n",
    "    \n",
    "    # Darstellen der Confusion Matrix durch Farben (hier grauöne)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    \n",
    "    # Darstellen der Fehler in Grautönen (Ablesen welche Klassen zu welcher fehlklassifiziert werden und reverse, oder eben nicht)\n",
    "    row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "    norm_conf_mx = conf_mx / row_sums\n",
    "    np.fill_diagonal(norm_conf_mx, 0)\n",
    "    plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreation CM Multiclass-Classification\n",
    "Die CM sollte soweit klar sien\n",
    "durch die \"plt.matshow\" kann ich gut erkennen welche Klasse wie gut klassifiziert wurde. Bspw. ist die 5 am dunkelsten => von den \"5\" wurde am meisten falsch klassifiziert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis auf Basis der Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
